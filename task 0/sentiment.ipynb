{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Task 1. Sentiment analysis\n",
    "# @author: dimitris.paraschakis@mah.se\n",
    "\n",
    "from sys import stdout\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from autocorrect import spell #https://github.com/phatpiglet/autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ae0670\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download Wordnet data\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with repeated characters\n",
    "def merge_multichars(token):    \n",
    "    multichars = {}\n",
    "    prev = token[0]\n",
    "    for char in token[1:]:\n",
    "        if char == prev:\n",
    "            if char not in multichars:\n",
    "                multichars[char] = 1\n",
    "            multichars[char] += 1\n",
    "        prev = char\n",
    "    for key,value in multichars.items():\n",
    "        if value > 3:\n",
    "            token = token.replace(key*value, key)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data for sentiment analysis\n",
    "def process(data):\n",
    "    sentiment_file = open(data)\n",
    "    st = EnglishStemmer()\n",
    "    lm = WordNetLemmatizer()\n",
    "    y = []\n",
    "    corpus = []\n",
    "    i = 0\n",
    "    for line in sentiment_file:\n",
    "        stdout.write('\\rprocessing row: %d...' % i)\n",
    "        stdout.flush()\n",
    "        sentence = ''\n",
    "        tokens = line.split('\\t')\n",
    "        y.append(int(tokens[0] == 'neg'))\n",
    "        tokens = tokens[1].split()\n",
    "        for token in tokens:\n",
    "            if not ('http' in token or '@' in token):\n",
    "                token = token.replace('&quot;', '')\n",
    "                subtokens = re.findall(r\"[\\w']+\", token)\n",
    "                for subtoken in subtokens:\n",
    "                    if len(subtoken) > 0:\n",
    "                        subtoken = merge_multichars(subtoken)\n",
    "                        subtoken = st.stem(lm.lemmatize(spell(subtoken)))\n",
    "                        sentence += subtoken+' '\n",
    "        if (sentence.strip()==''):\n",
    "            del y[i]\n",
    "            continue\n",
    "        i += 1\n",
    "        corpus.append(sentence.strip())\n",
    "    y = np.array(y, dtype=float)\n",
    "    stdout.write('\\n')\n",
    "    return np.array(corpus), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate scores based on TF-IDF encoding via cross-validation\n",
    "def cross_validate(corpus, y, model, folds):\n",
    "    splits = StratifiedKFold(y, folds, True)\n",
    "    y_pred = np.zeros_like(y, dtype=float)\n",
    "    for i_train, i_test in splits:\n",
    "        X_train = [corpus[i] for i in i_train]\n",
    "        X_test = [corpus[i] for i in i_test]\n",
    "        y_train = y[i_train]\n",
    "        vectorizer = TfidfVectorizer(sublinear_tf=True, use_idf=False)\n",
    "        X_train = vectorizer.fit_transform(X_train)\n",
    "        X_test = vectorizer.transform(X_test)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred[i_test] = model.predict_log_proba(X_test)[:,1]\n",
    "    return roc_auc_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing row: 1996...\n",
      "auc_roc = 0.805\n"
     ]
    }
   ],
   "source": [
    "# perform sentiment analysis of tweets\n",
    "corpus, y = process('data/sentiment.tsv')\n",
    "classifier = MultinomialNB(fit_prior=False)\n",
    "roc_auc = cross_validate(corpus, y, classifier, folds=10)\n",
    "print('auc_roc = %.3f' % (roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
